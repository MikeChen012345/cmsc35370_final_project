stage: predict

seed: 42
model_id: lmsys/vicuna-7b-v1.5
checkpoint: mindllm-base.ckpt
resume_id: null
group: null
run_name: null

lora: false
lr: 0.001
early_stop: false
output_dir: outputs/

defaults:
  - encoder: neuro_informed_attn

trainer:
  _target_: lightning.pytorch.Trainer
  accelerator: gpu
  devices: 1
  max_epochs: 2000
  check_val_every_n_epoch: 1
  accumulate_grad_batches: 8
  gradient_clip_val: 0.5
  use_distributed_sampler: false
  logger:
    _target_: lightning.pytorch.loggers.WandbLogger
    name: ${run_name}
    project: mindllm
    save_dir: ${output_dir}
    id: ${resume_id}
    group: null
  callbacks:
    - _target_: lightning.pytorch.callbacks.ModelCheckpoint
      monitor: val/token_loss
      filename: "{epoch:02d}"
    - _target_: lightning.pytorch.callbacks.LearningRateMonitor
      logging_interval: step
    - _target_: src.utils.callbacks.WatchModel

data:
  _target_: src.dataset.fMRIInstructionDataModule
  task: null
  exclude: null
  # If set, only these dataset sources will be used. Example: ["coco-caption", "coco-caption-previous", "paragraph-caption"]
  # Set to null to use all available sources.
  select_sources: ["coco-caption"]
  whole_brain: false
  num_workers: 0
  batch_size: 8
  mixup: false
  subjects:
    - 1
    # - 2
  train_samples: -1
  nsd_version: 0
  group_by_coco: true # if coco-caption-previous is used, this should be false; otherwise, set to true
  split_val: false
  split_seed: ${seed}

agent:
  # Enable the simple agent runner and configure the tool registry here.
  enabled: true
  # Default device for tools and model generation (overridable via CLI)
  device: gpu
  # Model generation control
  max_steps: 5
  max_new_tokens: 128
  # System prompt given to the model to follow the JSON tool-protocol
  system_prompt: >-
    You are an assistant for decoding the caption of a picture from fMRI embeddings.
    You may access some example embedding-to-caption pairs in your memory to help you predict captions.
    You may also need to incorporate checks for factual consistency and grammar based on your knowledge.
    
    Use the tools available and provide a final_answer for improved prediction when done.
    When you want to call a tool, reply with JSON {"action":"<tool>", "args":{...}}.
    When finished, always reply with {"final_answer":"..."} to indicate the end of the interaction.

    ONLY use the tools that are registered and available to you. If none is provided, DO NOT invent any tools.
    After receiving the tool output, you can choose to call another tool or provide the final answer.
    Always ensure your final answer is relevant to the caption prediction task.
  
  # Tool registry: enable/disable tools and provide default args for each tool.
  tools:
    memory:
      enabled: true
      # Number of previous embeddings to remember in the conversation history
      cross_subject: false # Whether to use the embedding of other subjects for memory search
      k: 5
    web_search:
      enabled: true
  # Multi-agent setup (currently only a critic agent is supported)
  multi_agent: 
    enbled: false

metrics:
  # If true, skip heavy COCO metrics that require external system dependencies
  # (SPICE, METEOR, CIDEr). Recommended on Windows or CI where Java/extra jars are unavailable.
  skip_heavy: true
  # Where to save a metrics JSON when running predict+eval. If empty, will save next to predictions file.
  output_metrics: ""